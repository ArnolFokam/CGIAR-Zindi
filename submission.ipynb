{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import sys \n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "from cgiar.utils import get_dir\n",
    "from cgiar.model import XCITMultipleMLP\n",
    "from cgiar.data import CGIARDataset_V4, augmentations\n",
    "\n",
    "# reduce font size of plots\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "LR=1e-4\n",
    "EPOCHS=30\n",
    "IMAGE_SIZE=224\n",
    "INITIAL_SIZE=512\n",
    "TRAIN_BATCH_SIZE=64\n",
    "TEST_BATCH_SIZE=32\n",
    "HIDDEN_SIZE=512\n",
    "NUM_FOLDS=5\n",
    "NUM_VIEWS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = get_dir('data/')\n",
    "ARTIFACTS = get_dir('solutions/v10/#1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE),\n",
    "    augmentations[\"RandomEqualize\"],\n",
    "    augmentations[\"RandomAffine\"],\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XCITMultipleMLP(\n",
    "    model_name=\"xcit_nano_12_p16_224\",\n",
    "    pretrained=True,\n",
    "    num_mlps=4,\n",
    "    hidden_size=HIDDEN_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(ARTIFACTS / f\"model_fold_{fold_idx}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XCITMultipleMLP(\n",
       "  (model): Xcit(\n",
       "    (patch_embed): ConvPatchEmbed(\n",
       "      (proj): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embed): PositionalEncodingFourier(\n",
       "      (token_projection): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x XCABlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): XCA(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm3): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (local_mp): LPI(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (act): GELU(approximate='none')\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "        )\n",
       "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls_attn_blocks): ModuleList(\n",
       "      (0-1): 2 x ClassAttentionBlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): ClassAttn(\n",
       "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    (head_drop): Dropout(p=0.2, inplace=False)\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data frame from csv\n",
    "X_test = pd.read_csv(DATA_DIR / 'Test.csv')\n",
    "\n",
    "test_images = CGIARDataset_V4.load_images(X_test, DATA_DIR / \"test\", INITIAL_SIZE)\n",
    "test_images = dict([test_images[idx] for idx in range(len(test_images))])\n",
    "\n",
    "# Construct test dataset and dataloader\n",
    "test_dataset = CGIARDataset_V4(\n",
    "    images=test_images,\n",
    "    num_views=NUM_VIEWS,\n",
    "    transform=transform,\n",
    "    features=X_test,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8663, 271)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271/271 [04:56<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for ids, images_list, growth_stage, season, _ in tqdm(test_loader):\n",
    "    \n",
    "        # average predictions from all the views\n",
    "        outputs = torch.stack([model((\n",
    "            growth_stage.to(device).squeeze(),\n",
    "            season.to(device).squeeze(),\n",
    "            images.to(device)\n",
    "        )) for images in images_list]).mean(dim=0)\n",
    "        \n",
    "        # get predictions from all the folds\n",
    "        outputs = outputs.tolist()\n",
    "        test_predictions.extend(list(zip(ids, outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sample submission file and update the extent column with the predictions\n",
    "submission_df = pd.read_csv('data/SampleSubmission.csv')\n",
    "\n",
    "# update the extent column with the predictions\n",
    "submission_df['extent'] = submission_df['ID'].map(dict(test_predictions))\n",
    "\n",
    "# save the submission file and trained model\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgiar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
